{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-neela/neela_INFO5731_spring2021/blob/main/Copy_of_INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84c14d7-4450-4690-a23e-b75b81225023"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "data = pd.read_csv('corona_vaccine.csv', low_memory = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "SA4ADBRRiWTj",
        "outputId": "61ddce72-08fe-4231-ad47-fff47aa3905c"
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "word_vectorizer = CountVectorizer(ngram_range = (3,3), analyzer = 'word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(data['tweets'].values.astype('U'))\n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>00 pm edt</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 casos de</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 v√≠timas da</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03 17 07</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03 2021 mortatily</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡Æ™‡Æü ‡Æé‡Æü ‡Æ∞‡Æ™</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡Æ™‡Æü ‡Æ§‡Æ™ ‡Æ™‡Æü</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡Æ∞‡Æ§‡ÆÆ‡Æ∞ ‡Æé‡Æö ‡Æö‡Æ∞</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡Æ∞‡Æ™ ‡Æ≥‡Æï ‡Æï‡ÆÆ</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡Æ≥‡Æï ‡Æï‡ÆÆ https</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1343 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Frequency\n",
              "00 pm edt                  1\n",
              "000 casos de               1\n",
              "000 v√≠timas da             1\n",
              "03 17 07                   1\n",
              "03 2021 mortatily          1\n",
              "...                      ...\n",
              "‡Æ™‡Æü ‡Æé‡Æü ‡Æ∞‡Æ™                   1\n",
              "‡Æ™‡Æü ‡Æ§‡Æ™ ‡Æ™‡Æü                   1\n",
              "‡Æ∞‡Æ§‡ÆÆ‡Æ∞ ‡Æé‡Æö ‡Æö‡Æ∞                 1\n",
              "‡Æ∞‡Æ™ ‡Æ≥‡Æï ‡Æï‡ÆÆ                   1\n",
              "‡Æ≥‡Æï ‡Æï‡ÆÆ https                1\n",
              "\n",
              "[1343 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZTBh6-tjGqi",
        "outputId": "ff60f56c-4f60-4521-9039-7572297233de"
      },
      "source": [
        "with open('corona_vaccine.csv') as a:\n",
        "  read = csv.reader(a)\n",
        "  next(read, None)\n",
        "  tweet = [row[1] for row in read]\n",
        "with open('tweets.txt', mode=\"w\") as c:\n",
        "  for b in tweet:\n",
        "    c.write(\"%s\\n\" % b)\n",
        "def bigram(file):\n",
        "  l1 = []\n",
        "  d = {}\n",
        "  u = {}\n",
        "  t = open(file, 'r').read()\n",
        "  l1 = t.strip().split()\n",
        "  del t\n",
        "  for f in l1:\n",
        "    if not f in u:\n",
        "      u[f] = 1\n",
        "    else:\n",
        "      u[f] += 1\n",
        "  for p in range(len(l1) - 1):\n",
        "    temp = (l1[p], l1[p+1])\n",
        "    if not temp in d:\n",
        "      b[temp] = 1\n",
        "    else:\n",
        "      b[temp] += 1\n",
        "  print('Generated', len(d), 'bigrams')\n",
        "  total = sum(u.values())\n",
        "  for n,t in d.items():\n",
        "    first_word = n[0]\n",
        "    first_wordcount = u[first_word]\n",
        "    bi = {}\n",
        "    bi = d[n]/u[first_word]\n",
        "    if t == 2:\n",
        "      print(n[0],n[1],t,bi)\n",
        "bigram('tweets.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated 1444 bigrams\n",
            "a la 2 0.15384615384615385\n",
            "\"No importa 2 0.6666666666666666\n",
            "importa que 2 1.0\n",
            "que sea 2 0.1\n",
            "sea la 2 1.0\n",
            "la rusa, 2 0.07142857142857142\n",
            "rusa, la 2 1.0\n",
            "la de 2 0.07142857142857142\n",
            "de Pfizer 2 0.03278688524590164\n",
            "Pfizer o 2 1.0\n",
            "o Moderna, 2 0.4\n",
            "Moderna, todas 2 1.0\n",
            "todas son 2 1.0\n",
            "son vacunas 2 0.6666666666666666\n",
            "vacunas seg‚Ä¶ 2 0.5\n",
            "#Salud | 2 1.0\n",
            "| 4.554 2 0.3333333333333333\n",
            "4.554 nuevos 2 1.0\n",
            "nuevos casos, 2 0.3333333333333333\n",
            "casos, 3.288 2 1.0\n",
            "3.288 recuperados 2 1.0\n",
            "recuperados y 2 0.6666666666666666\n",
            "y 130 2 0.13333333333333333\n",
            "130 muertes 2 1.0\n",
            "muertes en 2 0.6666666666666666\n",
            "en Colombia 2 0.06896551724137931\n",
            "Colombia a 2 1.0\n",
            "a causa 2 0.15384615384615385\n",
            "causa del 2 1.0\n",
            "del #coronavirus. 2 0.125\n",
            "#coronavirus. Lea 2 0.3333333333333333\n",
            "Lea la 2 1.0\n",
            "del #coronavirus 2 0.125\n",
            "#Coronav√≠rus: Mandetta 2 1.0\n",
            "Mandetta reage 2 1.0\n",
            "reage √†s 2 1.0\n",
            "√†s declara√ß√µes 2 1.0\n",
            "declara√ß√µes de 2 1.0\n",
            "de Guedes 2 0.03278688524590164\n",
            "Guedes sobre 2 1.0\n",
            "sobre atraso 2 0.6666666666666666\n",
            "atraso na 2 1.0\n",
            "na compra 2 1.0\n",
            "de vacinas: 2 0.03278688524590164\n",
            "vacinas: \"Desonesto 2 1.0\n",
            "\"Desonesto e 2 1.0\n",
            "e mentiroso\" 2 0.3333333333333333\n",
            "RT @hommel_b: 2 0.034482758620689655\n",
            "@hommel_b: https://t.co/tFfIleEiHu 2 1.0\n",
            "https://t.co/tFfIleEiHu via 2 1.0\n",
            "via @telegraaf 2 1.0\n",
            "@telegraaf Voor 2 1.0\n",
            "Voor alle 2 1.0\n",
            "alle @VVD 2 1.0\n",
            "@VVD en 2 1.0\n",
            "en @D66 2 0.06896551724137931\n",
            "@D66 stemmers... 2 1.0\n",
            "stemmers... Denkt 2 1.0\n",
            "Denkt u 2 1.0\n",
            "u wel 2 1.0\n",
            "wel om 2 1.0\n",
            "om uw 2 1.0\n",
            "uw 2e 2 1.0\n",
            "2e prik? 2 1.0\n",
            "prik? En 2 1.0\n",
            "En de 2 0.3333333333333333\n",
            "de 3e? 2 0.03278688524590164\n",
            "3e? En 2 1.0\n",
            "En plan 2 0.3333333333333333\n",
            "plan vo‚Ä¶ 2 1.0\n",
            "La farmac√©utica 2 0.25\n",
            "farmac√©utica #Roche 2 1.0\n",
            "#Roche lanz√≥ 2 1.0\n",
            "lanz√≥ un 2 1.0\n",
            "un test 2 0.16666666666666666\n",
            "test que 2 1.0\n",
            "que permite 2 0.1\n",
            "permite detectar 2 0.6666666666666666\n",
            "detectar las 2 1.0\n",
            "las diferentes 2 0.2\n",
            "diferentes #mutaciones 2 1.0\n",
            "#mutaciones en 2 0.6666666666666666\n",
            "las variantes 2 0.2\n",
            "variantes del‚Ä¶ 2 0.5\n",
            "el mundo 2 0.13333333333333333\n",
            "#Venezuela #MarisabelRodriguez 2 0.3333333333333333\n",
            "#MarisabelRodriguez @rahndono: 2 1.0\n",
            "@rahndono: \"¬øPor 2 1.0\n",
            "\"¬øPor qu√© 2 1.0\n",
            "qu√© no 2 1.0\n",
            "no Vacunan 2 0.4\n",
            "Vacunan a 2 1.0\n",
            "los odont√≥logos?. 2 0.2222222222222222\n",
            "odont√≥logos?. Somos 2 1.0\n",
            "Somos grupo 2 1.0\n",
            "grupo del 2 1.0\n",
            "del se‚Ä¶ 2 0.125\n",
            "con m√°s 2 0.2857142857142857\n",
            "la muerte 2 0.07142857142857142\n",
            "muerte de 2 1.0\n",
            "new cases 2 0.2857142857142857\n",
            "contra el 2 0.25\n",
            "RT @cmc435: 2 0.034482758620689655\n",
            "@cmc435: Remember, 2 1.0\n",
            "Remember, being 2 1.0\n",
            "being fully 2 1.0\n",
            "fully vaccinated 2 1.0\n",
            "vaccinated does 2 1.0\n",
            "does not 2 1.0\n",
            "not make 2 1.0\n",
            "make you 2 1.0\n",
            "you invincible 2 1.0\n",
            "invincible to 2 1.0\n",
            "to the 2 0.5\n",
            "the #coronavirus. 2 0.14285714285714285\n",
            "#coronavirus. You 2 0.3333333333333333\n",
            "You can 2 1.0\n",
            "can still 2 1.0\n",
            "still get 2 1.0\n",
            "get the 2 1.0\n",
            "the virus 2 0.14285714285714285\n",
            "virus and 2 0.6666666666666666\n",
            "and transmit 2 0.4\n",
            "transmit the‚Ä¶ 2 1.0\n",
            "the‚Ä¶ RT 2 1.0\n",
            "RT @theBreakerNews: 2 0.034482758620689655\n",
            "@theBreakerNews: ü¶†üö®UPDATE: 2 1.0\n",
            "4,851 activ‚Ä¶ 2 0.6666666666666666\n",
            "Si pudieras 2 1.0\n",
            "pudieras elegir 2 1.0\n",
            "elegir ¬øCu√°l 2 1.0\n",
            "¬øCu√°l te 2 1.0\n",
            "te dar√≠as 2 0.6666666666666666\n",
            "dar√≠as y 2 1.0\n",
            "y por 2 0.13333333333333333\n",
            "por qu√©? 2 0.2857142857142857\n",
            "qu√©? Te 2 1.0\n",
            "Te leemos. 2 1.0\n",
            "leemos. - 2 1.0\n",
            "- Fuente: 2 0.4\n",
            "Fuente: New 2 1.0\n",
            "New York 2 0.6666666666666666\n",
            "York Times 2 1.0\n",
            "Times #vacuna 2 1.0\n",
            "#vacuna #ensayosclinicos 2 0.6666666666666666\n",
            "RT @TV_Publica: 2 0.034482758620689655\n",
            "@TV_Publica: #Coronavirus 2 1.0\n",
            "en Argentina: 2 0.06896551724137931\n",
            "Argentina: A 2 1.0\n",
            "A trav√©s 2 0.6666666666666666\n",
            "trav√©s de 2 1.0\n",
            "de su 2 0.03278688524590164\n",
            "su reporte 2 0.6666666666666666\n",
            "reporte vespertino, 2 1.0\n",
            "vespertino, el 2 1.0\n",
            "el @msalnacion 2 0.13333333333333333\n",
            "@msalnacion inform√≥ 2 1.0\n",
            "inform√≥ que 2 1.0\n",
            "que se 2 0.1\n",
            "se registraron 2 0.3333333333333333\n",
            "registraron 8.304 2 1.0\n",
            "8.304 nuevos 2 1.0\n",
            "nuevos casos‚Ä¶ 2 0.3333333333333333\n",
            "#bitcoin‚Ä¶ RT 2 0.6666666666666666\n",
            "on the 2 0.6666666666666666\n",
            "cases in 2 0.5\n",
            "in the 2 0.25\n",
            "number of 2 1.0\n",
            "@CDWGWAGov: Managing 2 0.3333333333333333\n",
            "Managing Cloud 2 1.0\n",
            "Cloud Scale 2 1.0\n",
            "Scale and 2 1.0\n",
            "and Risks 2 0.4\n",
            "Risks Smartly 2 1.0\n",
            "Smartly #business 2 1.0\n",
            "#technology #COVID 2 0.6666666666666666\n",
            "#COVID #coronavirus 2 1.0\n",
            "#news #netops 2 0.6666666666666666\n",
            "#netops #100daysofcode 2 1.0\n",
            "#youtube #devo‚Ä¶ 2 0.3333333333333333\n",
            "#devo‚Ä¶ RT 2 1.0\n",
            "RT @Wonders_Always: 2 0.034482758620689655\n",
            "@Wonders_Always: De 2 1.0\n",
            "De bullshitcampagne 2 0.6666666666666666\n",
            "bullshitcampagne van 2 1.0\n",
            "van Thierry 2 1.0\n",
            "Thierry Baudet: 2 1.0\n",
            "Baudet: n√° 2 1.0\n",
            "n√° 17 2 1.0\n",
            "17 maart 2 1.0\n",
            "maart zijn 2 1.0\n",
            "zijn we 2 1.0\n",
            "we allemaal 2 0.6666666666666666\n",
            "allemaal vrij 2 1.0\n",
            "vrij en 2 1.0\n",
            "en is 2 0.06896551724137931\n",
            "is het 2 0.4\n",
            "het #coronavirus 2 1.0\n",
            "#coronavirus verdwenen 2 0.08\n",
            "verdwenen en 2 1.0\n",
            "en heffen 2 0.06896551724137931\n",
            "heffen we‚Ä¶ 2 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWBv-z-3lmu7",
        "outputId": "ed668b01-813e-42c2-9e8b-62daeea320cc"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "F2OI25ODlu_n",
        "outputId": "f2a26edd-6afd-46ce-cb8f-79f63a2ad284"
      },
      "source": [
        "nlp = spacy.load(\"en\")\n",
        "filename = open(\"tweets.txt\",\"r\")\n",
        "a = nlp(filename.read())\n",
        "noun = []\n",
        "for p in a.noun_chunks:\n",
        "  noun.append(p.text)\n",
        "df = pd.DataFrame(noun, columns=['noun'])\n",
        "word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(df['noun'].values.astype('U'))\n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df1 = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(),columns=['Frequency'])\n",
        "df1['NounProbabilities'] = df1/df1.max()\n",
        "print(noun)\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#Coronavirus', \"Perso j'ai\", 'Henry Mancini', \"de l'√™tre par le premier\", '#coronavirus', \"J'ai d√©couve\", 'African Affairs Radio', 'davido', 'Tune', '(Nonstop African Music', 'https://t.co/Qflfu6VkL0\\nüáß', 'üíâ', 'üá∫üá∏ La presi√≥n estadounidense', 'cuanto', 'volvi√≥ totalmente transparante al fi', 'RT @ReporteYa', '#Coronavirus #Venezuela', 'Dr.', 'No importa que sea', 'todas son', 'seg', 'RT @RedMasNoticias', '#', 'Salud | 4.554 nuevos casos', '3.288 recuperados', 'y 130 muertes', 'Colombia', 'a causa del #coronavirus', 'Lea la noticia ‚û°Ô∏è h', 'RT @qhusilu', 'Caso', 'M√°s casos de #coronavirus', 'en ancianos', '#CaserdeSanto√Ångel #Murcia', 'https://t.co/BSX2CGkNY6\\nCovid-19', 'en C√≥rdoba', 'la provincia', 'acerca', 'a los 170 mil positivos', '#CoronavirusEnCordoba https://t.co/tXdQgTwnRU', 'RT', 'Aprobaci√≥n/Desaprobaci√≥n del manejo', 'de la crisis del #coronavirus Trump y Biden', 'Es', 'posible mejorar', 'üá∫üá∏ e', '‚Ä¶\\nRT @Terra', '#Coronav√≠rus', 'Mandetta reage', '√†s declara√ß√µes de Guedes sobre', 'na compra de vacinas', 'https://t.co/gG', 'RT', 'https://t.co/tFfIleEiHu', 'Voor alle', '@D66 stemmers', 'Denkt u wel om uw 2e prik', 'En', 'de 3e', 'plan', '[EDUCACI√ìN', 'Durante la presentaci√≥n de la plataforma Cuidar Escuela', 'el ministro', 'de Educaci√≥n destac√≥ los cuidados', '#Roche lanz√≥ un test que permite detectar las', '#mutaciones', 'las variantes del', 'https://t.co/GYVcyROqV5', 'RT @ReporteYa', '#Coronavirus #Venezuela', 'Dr.', 'No hay que crearle falsas expectativas', 'a la gente', 'creo que 2021 seguir√°', 'Minist√©rio P√∫blico pede suspens√£o', 'de academias e sal√µes de beleza', 'em', 'Erechim', '‚Ä¶ https://t.co/Cr8ZqXKjph\\nRT', 'un proyecto de ley', 'Estes', 's√£o os dados atualizados da Covid-19', 'no Brasil e', 'no Rio Grande', 'nesta quarta-feira', '‚†Ä‚†Ä', 'La provincia toma medidas ante las nuevas', 'del Covid que circulan', 'el mundo https://t.co/LIW5Q3UVcr', 'RT @SET_Noticias', 'üíâüßì', 'El martes', 'la vacunaci√≥n contra', '#Coronavirus', 'nueve municipios de #Puebla', 'Reportaje', 'üì∞ #S', '9 de mar√ßo Santa Catarina chegava ao', 'Ap√≥s seis dias marcamos mai', 'https://t.co/ay93fL8dF0', 'RT', '@CMonteroOficial', 'un proyecto de ley', 'RT', 'Muere por #coronavirus el diputado brasile√±o que present√≥ un proyecto de ley contra', 'la vacunaci√≥n obligatoria de #Covi', 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', '#MarisabelRodriguez', '¬øPor', 'no Vacunan', 'Somos', 'La farmac√©utica', '#Roche lanz√≥ un test que permite detectar las', '#mutaciones', 'New COVID-19 Data', 'pm EDT', '#Coronavirus', '#COVID19', 'https://t.co/4Gt8MZaOOx\\nRT', '@martel_afd', 'Jetzt ist auch Alfred #Sauter', 'bayerischer Landtagsabgeordneter der #CSU', 'den #', 'Maskenskandal verwickelt', 'Ich warte darau', 'RT @ondavorace', 'Eccesso di mortalit√† 2020 rispetto alla media', 'e decessi', 'Covid-19 (su 1000 abitanti', 'RT', '|', 'Brasil registra', 'casos de covid', 'un solo d√≠a', '#coronavirus', \"L'objectif de #COVAX est de livrer\", '2 milliards', 'de doses de #', '√† 92 pays parmi les', 'https://t.co/hqmWd5fzcn', 'A Clean Home', 'a Happy Home', '#CleanProKSA', '#healthy #sanitization #ahealthyenvironment', 'https://t.co/2pF0NFJBe3', '#Salud | 4.554 nuevos casos', '3.288 recuperados', 'y 130 muertes', 'Colombia', 'a causa del #coronavirus', 'Lea la noticia', 'https://t.co/OVky2owQn0', '#Coronav√≠rus', 'Mandetta reage', '√†s declara√ß√µes de Guedes sobre', 'na compra de vacinas', 'https://t.co/gGtty5N0mF\\n#CORONAVIRUS', 'the home', 'medical conditions', 'virus symptoms', 'You', 'o Estado j√° havia apresentado uma crescente nos √≥bitos', '#RICMais', 'https://t.co/Fmw8XNexWn', 'Ô∏è Municipios del', '#EDOMEX', 'm√°s casos', 'de', '#COVID19ü¶†', 'üò∑', 'Hasta el d√≠a de hoy', 'se reportan', '225 mil 871 casos positivos', '#Coronavirus', 'Investigan', 'la muerte de una profesora', 'Marbella vacunada con AstraZeneca', '‡Æï‡Øä‡Æ∞‡Øã‡Æ©‡Ææ 2‡Æµ‡Æ§‡ØÅ ‡ÆÖ‡Æ≤‡Øà', '#Covid19', '#Coronavirus', 'https://t.co/ZUqn3nRZF3\\nRT @Sharmithashetty', '#coronavirus #COVID19updates\\n\\n- 28.9k new cases', 'new recoveries', '188 new deaths', 'active cases', '#Coronavirus', '#NTELEMICROPREGUNTA:¬øCree usted que los dominicanos', 'ofrecer√°n como', 'voluntarios para la Fase III de estudio de la', 'RT @jjosemussi', 'Vicenta D√≠az', 'vecina de #Berazategui', 'tiene', 'a√±os', 'y hoy recibi√≥', 'la primera dosis contra el #Coronavirus', 'el', 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', 'Dr.', 'Sigan con las medidas de prevenci√≥n de las que hemos hablado durante todo', 'RT', 'Ô∏èüì¢¬°#Qu√©dateEnCasa', 'üè† y prot√©gete', 'De un', 'no planeado', 'y del', '#Coronavirus', 'ü¶†', 'Disfruta', 'tu', '#COVID19', 'Covid_19 #CovidVaccine', '#coronavirus https://t.co/riwcljFanJ', 'RT', '@Nornenland', '#coronavirus', 'a fast slide', 'authoritarianism', 'the world', 'RT @AOK_Nordost', 'Erzieher', 'innen haben beim', '#Coronavirus das h√∂chste', '#Infektionsrisiko', '#Datenanalyse der AOK Nordost', 'So meldeten s', 'RT @cmc435', 'you', 'the #coronavirus', 'You', 'the virus', 'the‚Ä¶\\nRT', 'FFP2-Masken aus', 'Apotheken', 'Dumm und d√§mlich verdient', 'Bedenkt man', 'dass das Tragen dieser Masken f√ºr Privatpersonen unn√∂ti', 'RT @CDWGWAGov', ': VMware updates vRealize', 'portfolio', 'improved security', 'automation tools', '#100daysofcode #', '#youtube', '#btc', \"Today's COVID-19 newsletter\", 'some vaccine questions', 'a Maricopa County vaccination milestone', 'Over two million people', 'Scotland', 'their first dose', 'the #coronavirus vaccine', \"Scotland's biggest ever vaccinatio\", 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', 'Dr.', 'No importa que sea', 'la de Pfizer o Moderna', 'todas son', 'seg', '#coronavirus', 'Argentina', '195 muertes', 'y', 'nuevos contagios', 'mi√©rcoles', 'El total de infectados desde asciende', 'RT @theBreakerNews', 'ü¶†', 'üö®UPDATE', '#coronavirus', '-498', 'new cases', '4,851 activ', 'RT', 'Muere', 'por #coronavirus el diputado brasile√±o que present√≥ un proyecto de ley contra la vacun', 'RT', 'Matap√©dia', 'le', 'mars - La Matap√©dia', 'compte aucun cas actif depuis', 'le 9 f√©v', 'RT @OKAZ_online', 'üé• #', 'ÿßŸÑÿ∂Ÿàÿ° ÿπŸÑŸâ ÿ≤ÿ±ÿßÿπÿ© ÿßŸÑŸÖÿßŸÜÿ¨Ÿà', '#ÿ£ŸÖŸÑÿ¨', 'ÿ™ÿµŸàŸäÿ±', 'ÿπÿ®ÿØÿßŸÑŸÑŸá', 'RT @viniitj', 'Informa√ß√µes apontam', 'a vacina brasileira', 'vendedor de travesseiros e ministro nas horas vagas Marcos Pontes', 'RT @theBreakerNews', 'ü¶†', 'üö®UPDATE', '#coronavirus', '-498', 'new cases', '4,851 activ', '#Coronavirus', '¬ø', 'Las personas que', 'ya', 'fueron vacunadas contra el', 'barbijo y respetando la d', 'RT', '@hospimilTachira', '#LaCovidNoJuega Las', 'P.1 y P.2 del', 'capacidad', '#Coronavirus', '#Chubut', '115 nuevos casos', 'y', '3 fallecidos', 'en', 'las √∫ltimas', '24 horas', 'Le√©', 'm√°s', 'üîΩ', '‚Ä¶ https://t.co/TeDK8YdMbS', 'RT @UnEnsayoParaMi', 'Si pudieras', '¬ø', 'Te leemos', '-\\nFuente', 'New York Times', '#vacuna', '#ensayosclinicos', 'fiscal General del Estado de Quer√©taro', 'Alejandro Echeverr√≠a Cornejo', 'reconoci√≥ el esfuerzo', 'del personal‚Ä¶ https://t.co/J6ql8DJqJe\\nRT', '#Coronavirus', 'Argentina', 'A trav√©s de su reporte vespertino', 'el @msalnacion inform√≥ que', 'registraron', '8.304 nuevos casos', 'Si pudieras', '¬ø', 'Te leemos', '-\\nFuente', 'New York Times', '#vacuna', '#ensayosclinicos', 'https://t.co/PMS7Ggr04Y', 'Para lograr su aprobaci√≥n los reguladores revisan los resultados completos del ensayo', 'y los planes para la', 'Estas son las 9 vacunas', 'aprobadas', 'el mundo hasta ahora', '#vacuna', '#investigacion', '‚Ä¶ https://t.co/QPrHuM0ec1\\nRT', '@CDWGWAGov', ': VMware updates vRealize', 'portfolio', 'improved security', 'automation tools', '#100daysofcode #', '#youtube', '#btc', 'RT @ukiswitheu', '#coronavirus', 'it', 'the chin', 'we', 'people', 'love', '#Coronavirus', '¬ø', 'Nuevo s√≠ntoma', 'una joven', '√∫lcera vaginal dolorosa', '39 new cases', 'the Central African Republic', '[23:28 GMT', '#coronavirus', '#CoronaVirusUpdate #COVID19 #CoronavirusPandemic\\n‡Æï‡Æ∞‡Øç‡Æ®‡Ææ‡Æü‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç', '‡ÆÆ‡ØÄ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç', '‡Æä‡Æ∞‡Æü‡Æô‡Øç‡Æï‡ØÅ ‡ÆÖ‡ÆÆ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Ææ', '‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡ÆÆ‡Øç', 'RT', '@Llamadasos', 'obtener atenci√≥n m√©dica sin', '#Flexi', 'ü¶†üö®UPDATE', '#coronavirus', '-498', 'new cases', 'a‚Ä¶ https://t.co/JouEENEm3T\\nALERT', 'Half Hollow\\xa0Hills West football program', 'COVID-19 protocols - Global Pandemic News', '@Iran_GOV', 'reporters', 'the sidelines', \"the lat cabinet ministers' meeting\", 'the current Iranian calendar year', 'RT', '@cmc435', 'you', 'the #coronavirus', 'You', 'the virus', 'the‚Ä¶\\nRT @zemblanity00', '¬ø', 'C√≥mo las #mutaciones del #coronavirus', 'a eludir al sistema #inmune', 'https://t.co/WBeMmVbPwf', '11.700.431 infectados', '+90.830', '285.136 mortes', '10.261.154 recuperados', 'os', 'casos aumentaram', 'de', 'https://t.co/jXKR9s8oZY', 'Researchers', 'large number', '#COVID19 survivors', 'cognitive complications', 'RT', 'A petition', 'what', 'it', 'RT', 'un proyecto de ley', 'RT @CDWGWAGov', ': VMware updates vRealize', 'portfolio', 'improved security', 'automation tools', '#100daysofcode #', '#youtube', '#btc', 'RT', 'Vi√±eta n√∫mero', 'de la serie #HumorViral', 'https://t.co/q1ffsuatT4', '#vi√±etas', '#cor', 'RT', ': Ollanta Humala sobre paro de transportistas', 'El mensaje del presidente', 'de ruptura y provocaci√≥n para todos estos gremios', 'RT @CDWGWAGov', 'Cloud Scale', 'Risks', 'business #technology', 'COVID', 'Webcam Buying Guide', '#business #technology', 'coronavirus #news #100daysofcode #youtube #devops', '#code', '#cdwsocial https', 'RT', '#Coronavirus', 'Argentina', 'A trav√©s de su reporte vespertino', 'el @msalnacion inform√≥ que', 'registraron', '8.304 nuevos casos', 'RT', '@AliciaCastroAR', 'Lleg√≥', 'La', 'mas contagiosa y letal', 'RT', 'De bullshitcampagne van Thierry Baudet', 'n√° 17 maart zijn', 'we', 'allemaal vrij', 'heffen we', 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', '#MarisabelRodriguez', '¬øPor', 'no Vacunan', 'Somos', 'RT @CDWGWAGov', 'Cloud Scale', 'Risks', 'business #technology', 'COVID', 'https://t.co/tFfIleEiHu', 'Voor alle', '@D66 stemmers', 'Denkt u wel om uw 2e prik', 'En', 'de 3e', 'plan', 'RT @ReporteYa', '#Coronavirus', 'Se conoci√≥ de la muerte de Jos√© Gregorio Mar√≠n', 'trabajador del Hospital JM de los R√≠os', 'RT @SeYDuNa_21', '18-\\n\\n1 Mart', 'Bu', 'tarihe dair yazilanlar', \"g√ºn√ºm√ºz√ºn #Corona'sini anlatiyor sanki\", 'Acaba', 'mi planlanmis', '#coronavirus', 'En', 'C√≥rdoba', 'hay un', 'caso con la cepa de Manaos', 'En', 'Brasil casi', '2 mil muertos', '24 hs', 'Francia', 'The number', '#coronavirus cases', '#India', 'the death toll', 'Click', 'the lat', 'De bullshitcampagne van Thierry Baudet', 'n√° 17 maart zijn', 'we', 'allemaal vrij', 'heffen we', '#SwissCovidFail', '#TheWestCovidFail countries', 'what', 'https://t.co/yNTiyyh9tm']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "      <th>NounProbabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10 261 154</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100daysofcode youtube devops</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11 700 431</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115 nuevos casos</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17 maart zijn</th>\n",
              "      <td>2</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ÿßŸÑÿ∂Ÿàÿ° ÿπŸÑŸâ ÿ≤ÿ±ÿßÿπÿ©</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ÿπŸÑŸâ ÿ≤ÿ±ÿßÿπÿ© ÿßŸÑŸÖÿßŸÜÿ¨Ÿà</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡ÆÖ‡ÆÆ‡Æ≤ ‡Æ™‡Æü ‡Æ§‡Æ™</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡Æä‡Æ∞‡Æü‡Æô ‡ÆÖ‡ÆÆ‡Æ≤ ‡Æ™‡Æü</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>‡Æ™‡Æü ‡Æ§‡Æ™ ‡Æ™‡Æü</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>379 rows √ó 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Frequency  NounProbabilities\n",
              "10 261 154                            1                0.2\n",
              "100daysofcode youtube devops          1                0.2\n",
              "11 700 431                            1                0.2\n",
              "115 nuevos casos                      1                0.2\n",
              "17 maart zijn                         2                0.4\n",
              "...                                 ...                ...\n",
              "ÿßŸÑÿ∂Ÿàÿ° ÿπŸÑŸâ ÿ≤ÿ±ÿßÿπÿ©                       1                0.2\n",
              "ÿπŸÑŸâ ÿ≤ÿ±ÿßÿπÿ© ÿßŸÑŸÖÿßŸÜÿ¨Ÿà                     1                0.2\n",
              "‡ÆÖ‡ÆÆ‡Æ≤ ‡Æ™‡Æü ‡Æ§‡Æ™                             1                0.2\n",
              "‡Æä‡Æ∞‡Æü‡Æô ‡ÆÖ‡ÆÆ‡Æ≤ ‡Æ™‡Æü                           1                0.2\n",
              "‡Æ™‡Æü ‡Æ§‡Æ™ ‡Æ™‡Æü                              1                0.2\n",
              "\n",
              "[379 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "Giy46QF8H4I_",
        "outputId": "1dfa9b4d-c6e0-4dc4-e450-1f567483d2a3"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords \n",
        "df = pd.read_csv(\"corona_vaccine.csv\")\n",
        "tf2 = df.dropna()\n",
        "tf1 = (tf2['tweets'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index())\n",
        "tf1.columns = ['words','tf']\n",
        "for i , word in enumerate(tf1['words']):\n",
        "  tf1.loc[i,'idf'] = np.log(df.shape[0]/(len(tf2[tf2['tweets'].str.contains(word)])))\n",
        "tf1['tf*idf'] = tf1['tf'] * tf1['idf']\n",
        "import numpy.linalg as alg\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "train = tf2['tweets'].values.tolist()\n",
        "test = [\"Now playing on African Affairs Radio\"]\n",
        "stop = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(stop_words = stop)\n",
        "transformer = TfidfTransformer()\n",
        "array = vectorizer.fit_transform(train).toarray()\n",
        "tarray = vectorizer.transform(test).toarray()\n",
        "cx = lambda a,b : np.inner(a,b)/(alg.norm(a)*alg.norm(b))\n",
        "result = []\n",
        "for v in array:\n",
        "  for t in tarray:\n",
        "    cosine = cx(v, t)\n",
        "    result.append(cosine)\n",
        "p = tf2.filter(['No','tweets'], axis=1)\n",
        "s = pd.Series(result)\n",
        "p['Cosine_similarity'] = s.values\n",
        "p.drop(p.loc[p['Cosine_similarity']==0].index, inplace=True)\n",
        "p[\"Rank\"] = p[\"Cosine_similarity\"].rank().astype(int)\n",
        "p.sort_values(\"Cosine_similarity\", inplace=True)\n",
        "p"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>Cosine_similarity</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>39 new cases in the Central African Republic \\...</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Now playing on African Affairs Radio: if by da...</td>\n",
              "      <td>0.645497</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweets  Cosine_similarity  Rank\n",
              "71  39 new cases in the Central African Republic \\...           0.138675     1\n",
              "2   Now playing on African Affairs Radio: if by da...           0.645497     2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: https://github.com/s-neela/neela_INFO5731_spring2021/blob/main/Review.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}