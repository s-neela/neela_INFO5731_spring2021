{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-neela/neela_INFO5731_spring2021/blob/main/Copy_of_INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84c14d7-4450-4690-a23e-b75b81225023"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "data = pd.read_csv('corona_vaccine.csv', low_memory = False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "SA4ADBRRiWTj",
        "outputId": "61ddce72-08fe-4231-ad47-fff47aa3905c"
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "word_vectorizer = CountVectorizer(ngram_range = (3,3), analyzer = 'word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(data['tweets'].values.astype('U'))\n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>00 pm edt</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 casos de</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000 vítimas da</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03 17 07</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03 2021 mortatily</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>பட எட ரப</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>பட தப பட</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ரதமர எச சர</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ரப ளக கம</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ளக கம https</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1343 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Frequency\n",
              "00 pm edt                  1\n",
              "000 casos de               1\n",
              "000 vítimas da             1\n",
              "03 17 07                   1\n",
              "03 2021 mortatily          1\n",
              "...                      ...\n",
              "பட எட ரப                   1\n",
              "பட தப பட                   1\n",
              "ரதமர எச சர                 1\n",
              "ரப ளக கம                   1\n",
              "ளக கம https                1\n",
              "\n",
              "[1343 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZTBh6-tjGqi",
        "outputId": "ff60f56c-4f60-4521-9039-7572297233de"
      },
      "source": [
        "with open('corona_vaccine.csv') as a:\n",
        "  read = csv.reader(a)\n",
        "  next(read, None)\n",
        "  tweet = [row[1] for row in read]\n",
        "with open('tweets.txt', mode=\"w\") as c:\n",
        "  for b in tweet:\n",
        "    c.write(\"%s\\n\" % b)\n",
        "def bigram(file):\n",
        "  l1 = []\n",
        "  d = {}\n",
        "  u = {}\n",
        "  t = open(file, 'r').read()\n",
        "  l1 = t.strip().split()\n",
        "  del t\n",
        "  for f in l1:\n",
        "    if not f in u:\n",
        "      u[f] = 1\n",
        "    else:\n",
        "      u[f] += 1\n",
        "  for p in range(len(l1) - 1):\n",
        "    temp = (l1[p], l1[p+1])\n",
        "    if not temp in d:\n",
        "      b[temp] = 1\n",
        "    else:\n",
        "      b[temp] += 1\n",
        "  print('Generated', len(d), 'bigrams')\n",
        "  total = sum(u.values())\n",
        "  for n,t in d.items():\n",
        "    first_word = n[0]\n",
        "    first_wordcount = u[first_word]\n",
        "    bi = {}\n",
        "    bi = d[n]/u[first_word]\n",
        "    if t == 2:\n",
        "      print(n[0],n[1],t,bi)\n",
        "bigram('tweets.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated 1444 bigrams\n",
            "a la 2 0.15384615384615385\n",
            "\"No importa 2 0.6666666666666666\n",
            "importa que 2 1.0\n",
            "que sea 2 0.1\n",
            "sea la 2 1.0\n",
            "la rusa, 2 0.07142857142857142\n",
            "rusa, la 2 1.0\n",
            "la de 2 0.07142857142857142\n",
            "de Pfizer 2 0.03278688524590164\n",
            "Pfizer o 2 1.0\n",
            "o Moderna, 2 0.4\n",
            "Moderna, todas 2 1.0\n",
            "todas son 2 1.0\n",
            "son vacunas 2 0.6666666666666666\n",
            "vacunas seg… 2 0.5\n",
            "#Salud | 2 1.0\n",
            "| 4.554 2 0.3333333333333333\n",
            "4.554 nuevos 2 1.0\n",
            "nuevos casos, 2 0.3333333333333333\n",
            "casos, 3.288 2 1.0\n",
            "3.288 recuperados 2 1.0\n",
            "recuperados y 2 0.6666666666666666\n",
            "y 130 2 0.13333333333333333\n",
            "130 muertes 2 1.0\n",
            "muertes en 2 0.6666666666666666\n",
            "en Colombia 2 0.06896551724137931\n",
            "Colombia a 2 1.0\n",
            "a causa 2 0.15384615384615385\n",
            "causa del 2 1.0\n",
            "del #coronavirus. 2 0.125\n",
            "#coronavirus. Lea 2 0.3333333333333333\n",
            "Lea la 2 1.0\n",
            "del #coronavirus 2 0.125\n",
            "#Coronavírus: Mandetta 2 1.0\n",
            "Mandetta reage 2 1.0\n",
            "reage às 2 1.0\n",
            "às declarações 2 1.0\n",
            "declarações de 2 1.0\n",
            "de Guedes 2 0.03278688524590164\n",
            "Guedes sobre 2 1.0\n",
            "sobre atraso 2 0.6666666666666666\n",
            "atraso na 2 1.0\n",
            "na compra 2 1.0\n",
            "de vacinas: 2 0.03278688524590164\n",
            "vacinas: \"Desonesto 2 1.0\n",
            "\"Desonesto e 2 1.0\n",
            "e mentiroso\" 2 0.3333333333333333\n",
            "RT @hommel_b: 2 0.034482758620689655\n",
            "@hommel_b: https://t.co/tFfIleEiHu 2 1.0\n",
            "https://t.co/tFfIleEiHu via 2 1.0\n",
            "via @telegraaf 2 1.0\n",
            "@telegraaf Voor 2 1.0\n",
            "Voor alle 2 1.0\n",
            "alle @VVD 2 1.0\n",
            "@VVD en 2 1.0\n",
            "en @D66 2 0.06896551724137931\n",
            "@D66 stemmers... 2 1.0\n",
            "stemmers... Denkt 2 1.0\n",
            "Denkt u 2 1.0\n",
            "u wel 2 1.0\n",
            "wel om 2 1.0\n",
            "om uw 2 1.0\n",
            "uw 2e 2 1.0\n",
            "2e prik? 2 1.0\n",
            "prik? En 2 1.0\n",
            "En de 2 0.3333333333333333\n",
            "de 3e? 2 0.03278688524590164\n",
            "3e? En 2 1.0\n",
            "En plan 2 0.3333333333333333\n",
            "plan vo… 2 1.0\n",
            "La farmacéutica 2 0.25\n",
            "farmacéutica #Roche 2 1.0\n",
            "#Roche lanzó 2 1.0\n",
            "lanzó un 2 1.0\n",
            "un test 2 0.16666666666666666\n",
            "test que 2 1.0\n",
            "que permite 2 0.1\n",
            "permite detectar 2 0.6666666666666666\n",
            "detectar las 2 1.0\n",
            "las diferentes 2 0.2\n",
            "diferentes #mutaciones 2 1.0\n",
            "#mutaciones en 2 0.6666666666666666\n",
            "las variantes 2 0.2\n",
            "variantes del… 2 0.5\n",
            "el mundo 2 0.13333333333333333\n",
            "#Venezuela #MarisabelRodriguez 2 0.3333333333333333\n",
            "#MarisabelRodriguez @rahndono: 2 1.0\n",
            "@rahndono: \"¿Por 2 1.0\n",
            "\"¿Por qué 2 1.0\n",
            "qué no 2 1.0\n",
            "no Vacunan 2 0.4\n",
            "Vacunan a 2 1.0\n",
            "los odontólogos?. 2 0.2222222222222222\n",
            "odontólogos?. Somos 2 1.0\n",
            "Somos grupo 2 1.0\n",
            "grupo del 2 1.0\n",
            "del se… 2 0.125\n",
            "con más 2 0.2857142857142857\n",
            "la muerte 2 0.07142857142857142\n",
            "muerte de 2 1.0\n",
            "new cases 2 0.2857142857142857\n",
            "contra el 2 0.25\n",
            "RT @cmc435: 2 0.034482758620689655\n",
            "@cmc435: Remember, 2 1.0\n",
            "Remember, being 2 1.0\n",
            "being fully 2 1.0\n",
            "fully vaccinated 2 1.0\n",
            "vaccinated does 2 1.0\n",
            "does not 2 1.0\n",
            "not make 2 1.0\n",
            "make you 2 1.0\n",
            "you invincible 2 1.0\n",
            "invincible to 2 1.0\n",
            "to the 2 0.5\n",
            "the #coronavirus. 2 0.14285714285714285\n",
            "#coronavirus. You 2 0.3333333333333333\n",
            "You can 2 1.0\n",
            "can still 2 1.0\n",
            "still get 2 1.0\n",
            "get the 2 1.0\n",
            "the virus 2 0.14285714285714285\n",
            "virus and 2 0.6666666666666666\n",
            "and transmit 2 0.4\n",
            "transmit the… 2 1.0\n",
            "the… RT 2 1.0\n",
            "RT @theBreakerNews: 2 0.034482758620689655\n",
            "@theBreakerNews: 🦠🚨UPDATE: 2 1.0\n",
            "4,851 activ… 2 0.6666666666666666\n",
            "Si pudieras 2 1.0\n",
            "pudieras elegir 2 1.0\n",
            "elegir ¿Cuál 2 1.0\n",
            "¿Cuál te 2 1.0\n",
            "te darías 2 0.6666666666666666\n",
            "darías y 2 1.0\n",
            "y por 2 0.13333333333333333\n",
            "por qué? 2 0.2857142857142857\n",
            "qué? Te 2 1.0\n",
            "Te leemos. 2 1.0\n",
            "leemos. - 2 1.0\n",
            "- Fuente: 2 0.4\n",
            "Fuente: New 2 1.0\n",
            "New York 2 0.6666666666666666\n",
            "York Times 2 1.0\n",
            "Times #vacuna 2 1.0\n",
            "#vacuna #ensayosclinicos 2 0.6666666666666666\n",
            "RT @TV_Publica: 2 0.034482758620689655\n",
            "@TV_Publica: #Coronavirus 2 1.0\n",
            "en Argentina: 2 0.06896551724137931\n",
            "Argentina: A 2 1.0\n",
            "A través 2 0.6666666666666666\n",
            "través de 2 1.0\n",
            "de su 2 0.03278688524590164\n",
            "su reporte 2 0.6666666666666666\n",
            "reporte vespertino, 2 1.0\n",
            "vespertino, el 2 1.0\n",
            "el @msalnacion 2 0.13333333333333333\n",
            "@msalnacion informó 2 1.0\n",
            "informó que 2 1.0\n",
            "que se 2 0.1\n",
            "se registraron 2 0.3333333333333333\n",
            "registraron 8.304 2 1.0\n",
            "8.304 nuevos 2 1.0\n",
            "nuevos casos… 2 0.3333333333333333\n",
            "#bitcoin… RT 2 0.6666666666666666\n",
            "on the 2 0.6666666666666666\n",
            "cases in 2 0.5\n",
            "in the 2 0.25\n",
            "number of 2 1.0\n",
            "@CDWGWAGov: Managing 2 0.3333333333333333\n",
            "Managing Cloud 2 1.0\n",
            "Cloud Scale 2 1.0\n",
            "Scale and 2 1.0\n",
            "and Risks 2 0.4\n",
            "Risks Smartly 2 1.0\n",
            "Smartly #business 2 1.0\n",
            "#technology #COVID 2 0.6666666666666666\n",
            "#COVID #coronavirus 2 1.0\n",
            "#news #netops 2 0.6666666666666666\n",
            "#netops #100daysofcode 2 1.0\n",
            "#youtube #devo… 2 0.3333333333333333\n",
            "#devo… RT 2 1.0\n",
            "RT @Wonders_Always: 2 0.034482758620689655\n",
            "@Wonders_Always: De 2 1.0\n",
            "De bullshitcampagne 2 0.6666666666666666\n",
            "bullshitcampagne van 2 1.0\n",
            "van Thierry 2 1.0\n",
            "Thierry Baudet: 2 1.0\n",
            "Baudet: ná 2 1.0\n",
            "ná 17 2 1.0\n",
            "17 maart 2 1.0\n",
            "maart zijn 2 1.0\n",
            "zijn we 2 1.0\n",
            "we allemaal 2 0.6666666666666666\n",
            "allemaal vrij 2 1.0\n",
            "vrij en 2 1.0\n",
            "en is 2 0.06896551724137931\n",
            "is het 2 0.4\n",
            "het #coronavirus 2 1.0\n",
            "#coronavirus verdwenen 2 0.08\n",
            "verdwenen en 2 1.0\n",
            "en heffen 2 0.06896551724137931\n",
            "heffen we… 2 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWBv-z-3lmu7",
        "outputId": "ed668b01-813e-42c2-9e8b-62daeea320cc"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "F2OI25ODlu_n",
        "outputId": "f2a26edd-6afd-46ce-cb8f-79f63a2ad284"
      },
      "source": [
        "nlp = spacy.load(\"en\")\n",
        "filename = open(\"tweets.txt\",\"r\")\n",
        "a = nlp(filename.read())\n",
        "noun = []\n",
        "for p in a.noun_chunks:\n",
        "  noun.append(p.text)\n",
        "df = pd.DataFrame(noun, columns=['noun'])\n",
        "word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(df['noun'].values.astype('U'))\n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df1 = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(),columns=['Frequency'])\n",
        "df1['NounProbabilities'] = df1/df1.max()\n",
        "print(noun)\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#Coronavirus', \"Perso j'ai\", 'Henry Mancini', \"de l'être par le premier\", '#coronavirus', \"J'ai découve\", 'African Affairs Radio', 'davido', 'Tune', '(Nonstop African Music', 'https://t.co/Qflfu6VkL0\\n🇧', '💉', '🇺🇸 La presión estadounidense', 'cuanto', 'volvió totalmente transparante al fi', 'RT @ReporteYa', '#Coronavirus #Venezuela', 'Dr.', 'No importa que sea', 'todas son', 'seg', 'RT @RedMasNoticias', '#', 'Salud | 4.554 nuevos casos', '3.288 recuperados', 'y 130 muertes', 'Colombia', 'a causa del #coronavirus', 'Lea la noticia ➡️ h', 'RT @qhusilu', 'Caso', 'Más casos de #coronavirus', 'en ancianos', '#CaserdeSantoÁngel #Murcia', 'https://t.co/BSX2CGkNY6\\nCovid-19', 'en Córdoba', 'la provincia', 'acerca', 'a los 170 mil positivos', '#CoronavirusEnCordoba https://t.co/tXdQgTwnRU', 'RT', 'Aprobación/Desaprobación del manejo', 'de la crisis del #coronavirus Trump y Biden', 'Es', 'posible mejorar', '🇺🇸 e', '…\\nRT @Terra', '#Coronavírus', 'Mandetta reage', 'às declarações de Guedes sobre', 'na compra de vacinas', 'https://t.co/gG', 'RT', 'https://t.co/tFfIleEiHu', 'Voor alle', '@D66 stemmers', 'Denkt u wel om uw 2e prik', 'En', 'de 3e', 'plan', '[EDUCACIÓN', 'Durante la presentación de la plataforma Cuidar Escuela', 'el ministro', 'de Educación destacó los cuidados', '#Roche lanzó un test que permite detectar las', '#mutaciones', 'las variantes del', 'https://t.co/GYVcyROqV5', 'RT @ReporteYa', '#Coronavirus #Venezuela', 'Dr.', 'No hay que crearle falsas expectativas', 'a la gente', 'creo que 2021 seguirá', 'Ministério Público pede suspensão', 'de academias e salões de beleza', 'em', 'Erechim', '… https://t.co/Cr8ZqXKjph\\nRT', 'un proyecto de ley', 'Estes', 'são os dados atualizados da Covid-19', 'no Brasil e', 'no Rio Grande', 'nesta quarta-feira', '⠀⠀', 'La provincia toma medidas ante las nuevas', 'del Covid que circulan', 'el mundo https://t.co/LIW5Q3UVcr', 'RT @SET_Noticias', '💉🧓', 'El martes', 'la vacunación contra', '#Coronavirus', 'nueve municipios de #Puebla', 'Reportaje', '📰 #S', '9 de março Santa Catarina chegava ao', 'Após seis dias marcamos mai', 'https://t.co/ay93fL8dF0', 'RT', '@CMonteroOficial', 'un proyecto de ley', 'RT', 'Muere por #coronavirus el diputado brasileño que presentó un proyecto de ley contra', 'la vacunación obligatoria de #Covi', 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', '#MarisabelRodriguez', '¿Por', 'no Vacunan', 'Somos', 'La farmacéutica', '#Roche lanzó un test que permite detectar las', '#mutaciones', 'New COVID-19 Data', 'pm EDT', '#Coronavirus', '#COVID19', 'https://t.co/4Gt8MZaOOx\\nRT', '@martel_afd', 'Jetzt ist auch Alfred #Sauter', 'bayerischer Landtagsabgeordneter der #CSU', 'den #', 'Maskenskandal verwickelt', 'Ich warte darau', 'RT @ondavorace', 'Eccesso di mortalità 2020 rispetto alla media', 'e decessi', 'Covid-19 (su 1000 abitanti', 'RT', '|', 'Brasil registra', 'casos de covid', 'un solo día', '#coronavirus', \"L'objectif de #COVAX est de livrer\", '2 milliards', 'de doses de #', 'à 92 pays parmi les', 'https://t.co/hqmWd5fzcn', 'A Clean Home', 'a Happy Home', '#CleanProKSA', '#healthy #sanitization #ahealthyenvironment', 'https://t.co/2pF0NFJBe3', '#Salud | 4.554 nuevos casos', '3.288 recuperados', 'y 130 muertes', 'Colombia', 'a causa del #coronavirus', 'Lea la noticia', 'https://t.co/OVky2owQn0', '#Coronavírus', 'Mandetta reage', 'às declarações de Guedes sobre', 'na compra de vacinas', 'https://t.co/gGtty5N0mF\\n#CORONAVIRUS', 'the home', 'medical conditions', 'virus symptoms', 'You', 'o Estado já havia apresentado uma crescente nos óbitos', '#RICMais', 'https://t.co/Fmw8XNexWn', '️ Municipios del', '#EDOMEX', 'más casos', 'de', '#COVID19🦠', '😷', 'Hasta el día de hoy', 'se reportan', '225 mil 871 casos positivos', '#Coronavirus', 'Investigan', 'la muerte de una profesora', 'Marbella vacunada con AstraZeneca', 'கொரோனா 2வது அலை', '#Covid19', '#Coronavirus', 'https://t.co/ZUqn3nRZF3\\nRT @Sharmithashetty', '#coronavirus #COVID19updates\\n\\n- 28.9k new cases', 'new recoveries', '188 new deaths', 'active cases', '#Coronavirus', '#NTELEMICROPREGUNTA:¿Cree usted que los dominicanos', 'ofrecerán como', 'voluntarios para la Fase III de estudio de la', 'RT @jjosemussi', 'Vicenta Díaz', 'vecina de #Berazategui', 'tiene', 'años', 'y hoy recibió', 'la primera dosis contra el #Coronavirus', 'el', 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', 'Dr.', 'Sigan con las medidas de prevención de las que hemos hablado durante todo', 'RT', '️📢¡#QuédateEnCasa', '🏠 y protégete', 'De un', 'no planeado', 'y del', '#Coronavirus', '🦠', 'Disfruta', 'tu', '#COVID19', 'Covid_19 #CovidVaccine', '#coronavirus https://t.co/riwcljFanJ', 'RT', '@Nornenland', '#coronavirus', 'a fast slide', 'authoritarianism', 'the world', 'RT @AOK_Nordost', 'Erzieher', 'innen haben beim', '#Coronavirus das höchste', '#Infektionsrisiko', '#Datenanalyse der AOK Nordost', 'So meldeten s', 'RT @cmc435', 'you', 'the #coronavirus', 'You', 'the virus', 'the…\\nRT', 'FFP2-Masken aus', 'Apotheken', 'Dumm und dämlich verdient', 'Bedenkt man', 'dass das Tragen dieser Masken für Privatpersonen unnöti', 'RT @CDWGWAGov', ': VMware updates vRealize', 'portfolio', 'improved security', 'automation tools', '#100daysofcode #', '#youtube', '#btc', \"Today's COVID-19 newsletter\", 'some vaccine questions', 'a Maricopa County vaccination milestone', 'Over two million people', 'Scotland', 'their first dose', 'the #coronavirus vaccine', \"Scotland's biggest ever vaccinatio\", 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', 'Dr.', 'No importa que sea', 'la de Pfizer o Moderna', 'todas son', 'seg', '#coronavirus', 'Argentina', '195 muertes', 'y', 'nuevos contagios', 'miércoles', 'El total de infectados desde asciende', 'RT @theBreakerNews', '🦠', '🚨UPDATE', '#coronavirus', '-498', 'new cases', '4,851 activ', 'RT', 'Muere', 'por #coronavirus el diputado brasileño que presentó un proyecto de ley contra la vacun', 'RT', 'Matapédia', 'le', 'mars - La Matapédia', 'compte aucun cas actif depuis', 'le 9 fév', 'RT @OKAZ_online', '🎥 #', 'الضوء على زراعة المانجو', '#أملج', 'تصوير', 'عبدالله', 'RT @viniitj', 'Informações apontam', 'a vacina brasileira', 'vendedor de travesseiros e ministro nas horas vagas Marcos Pontes', 'RT @theBreakerNews', '🦠', '🚨UPDATE', '#coronavirus', '-498', 'new cases', '4,851 activ', '#Coronavirus', '¿', 'Las personas que', 'ya', 'fueron vacunadas contra el', 'barbijo y respetando la d', 'RT', '@hospimilTachira', '#LaCovidNoJuega Las', 'P.1 y P.2 del', 'capacidad', '#Coronavirus', '#Chubut', '115 nuevos casos', 'y', '3 fallecidos', 'en', 'las últimas', '24 horas', 'Leé', 'más', '🔽', '… https://t.co/TeDK8YdMbS', 'RT @UnEnsayoParaMi', 'Si pudieras', '¿', 'Te leemos', '-\\nFuente', 'New York Times', '#vacuna', '#ensayosclinicos', 'fiscal General del Estado de Querétaro', 'Alejandro Echeverría Cornejo', 'reconoció el esfuerzo', 'del personal… https://t.co/J6ql8DJqJe\\nRT', '#Coronavirus', 'Argentina', 'A través de su reporte vespertino', 'el @msalnacion informó que', 'registraron', '8.304 nuevos casos', 'Si pudieras', '¿', 'Te leemos', '-\\nFuente', 'New York Times', '#vacuna', '#ensayosclinicos', 'https://t.co/PMS7Ggr04Y', 'Para lograr su aprobación los reguladores revisan los resultados completos del ensayo', 'y los planes para la', 'Estas son las 9 vacunas', 'aprobadas', 'el mundo hasta ahora', '#vacuna', '#investigacion', '… https://t.co/QPrHuM0ec1\\nRT', '@CDWGWAGov', ': VMware updates vRealize', 'portfolio', 'improved security', 'automation tools', '#100daysofcode #', '#youtube', '#btc', 'RT @ukiswitheu', '#coronavirus', 'it', 'the chin', 'we', 'people', 'love', '#Coronavirus', '¿', 'Nuevo síntoma', 'una joven', 'úlcera vaginal dolorosa', '39 new cases', 'the Central African Republic', '[23:28 GMT', '#coronavirus', '#CoronaVirusUpdate #COVID19 #CoronavirusPandemic\\nகர்நாடகத்தில்', 'மீண்டும்', 'ஊரடங்கு அமல்படுத்தப்படுமா', 'விளக்கம்', 'RT', '@Llamadasos', 'obtener atención médica sin', '#Flexi', '🦠🚨UPDATE', '#coronavirus', '-498', 'new cases', 'a… https://t.co/JouEENEm3T\\nALERT', 'Half Hollow\\xa0Hills West football program', 'COVID-19 protocols - Global Pandemic News', '@Iran_GOV', 'reporters', 'the sidelines', \"the lat cabinet ministers' meeting\", 'the current Iranian calendar year', 'RT', '@cmc435', 'you', 'the #coronavirus', 'You', 'the virus', 'the…\\nRT @zemblanity00', '¿', 'Cómo las #mutaciones del #coronavirus', 'a eludir al sistema #inmune', 'https://t.co/WBeMmVbPwf', '11.700.431 infectados', '+90.830', '285.136 mortes', '10.261.154 recuperados', 'os', 'casos aumentaram', 'de', 'https://t.co/jXKR9s8oZY', 'Researchers', 'large number', '#COVID19 survivors', 'cognitive complications', 'RT', 'A petition', 'what', 'it', 'RT', 'un proyecto de ley', 'RT @CDWGWAGov', ': VMware updates vRealize', 'portfolio', 'improved security', 'automation tools', '#100daysofcode #', '#youtube', '#btc', 'RT', 'Viñeta número', 'de la serie #HumorViral', 'https://t.co/q1ffsuatT4', '#viñetas', '#cor', 'RT', ': Ollanta Humala sobre paro de transportistas', 'El mensaje del presidente', 'de ruptura y provocación para todos estos gremios', 'RT @CDWGWAGov', 'Cloud Scale', 'Risks', 'business #technology', 'COVID', 'Webcam Buying Guide', '#business #technology', 'coronavirus #news #100daysofcode #youtube #devops', '#code', '#cdwsocial https', 'RT', '#Coronavirus', 'Argentina', 'A través de su reporte vespertino', 'el @msalnacion informó que', 'registraron', '8.304 nuevos casos', 'RT', '@AliciaCastroAR', 'Llegó', 'La', 'mas contagiosa y letal', 'RT', 'De bullshitcampagne van Thierry Baudet', 'ná 17 maart zijn', 'we', 'allemaal vrij', 'heffen we', 'RT @ReporteYa', '#17Mar #Coronavirus #Venezuela', '#MarisabelRodriguez', '¿Por', 'no Vacunan', 'Somos', 'RT @CDWGWAGov', 'Cloud Scale', 'Risks', 'business #technology', 'COVID', 'https://t.co/tFfIleEiHu', 'Voor alle', '@D66 stemmers', 'Denkt u wel om uw 2e prik', 'En', 'de 3e', 'plan', 'RT @ReporteYa', '#Coronavirus', 'Se conoció de la muerte de José Gregorio Marín', 'trabajador del Hospital JM de los Ríos', 'RT @SeYDuNa_21', '18-\\n\\n1 Mart', 'Bu', 'tarihe dair yazilanlar', \"günümüzün #Corona'sini anlatiyor sanki\", 'Acaba', 'mi planlanmis', '#coronavirus', 'En', 'Córdoba', 'hay un', 'caso con la cepa de Manaos', 'En', 'Brasil casi', '2 mil muertos', '24 hs', 'Francia', 'The number', '#coronavirus cases', '#India', 'the death toll', 'Click', 'the lat', 'De bullshitcampagne van Thierry Baudet', 'ná 17 maart zijn', 'we', 'allemaal vrij', 'heffen we', '#SwissCovidFail', '#TheWestCovidFail countries', 'what', 'https://t.co/yNTiyyh9tm']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "      <th>NounProbabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10 261 154</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100daysofcode youtube devops</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11 700 431</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115 nuevos casos</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17 maart zijn</th>\n",
              "      <td>2</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>الضوء على زراعة</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>على زراعة المانجو</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>அமல பட தப</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ஊரடங அமல பட</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>பட தப பட</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>379 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Frequency  NounProbabilities\n",
              "10 261 154                            1                0.2\n",
              "100daysofcode youtube devops          1                0.2\n",
              "11 700 431                            1                0.2\n",
              "115 nuevos casos                      1                0.2\n",
              "17 maart zijn                         2                0.4\n",
              "...                                 ...                ...\n",
              "الضوء على زراعة                       1                0.2\n",
              "على زراعة المانجو                     1                0.2\n",
              "அமல பட தப                             1                0.2\n",
              "ஊரடங அமல பட                           1                0.2\n",
              "பட தப பட                              1                0.2\n",
              "\n",
              "[379 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "Giy46QF8H4I_",
        "outputId": "1dfa9b4d-c6e0-4dc4-e450-1f567483d2a3"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords \n",
        "df = pd.read_csv(\"corona_vaccine.csv\")\n",
        "tf2 = df.dropna()\n",
        "tf1 = (tf2['tweets'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index())\n",
        "tf1.columns = ['words','tf']\n",
        "for i , word in enumerate(tf1['words']):\n",
        "  tf1.loc[i,'idf'] = np.log(df.shape[0]/(len(tf2[tf2['tweets'].str.contains(word)])))\n",
        "tf1['tf*idf'] = tf1['tf'] * tf1['idf']\n",
        "import numpy.linalg as alg\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "train = tf2['tweets'].values.tolist()\n",
        "test = [\"Now playing on African Affairs Radio\"]\n",
        "stop = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(stop_words = stop)\n",
        "transformer = TfidfTransformer()\n",
        "array = vectorizer.fit_transform(train).toarray()\n",
        "tarray = vectorizer.transform(test).toarray()\n",
        "cx = lambda a,b : np.inner(a,b)/(alg.norm(a)*alg.norm(b))\n",
        "result = []\n",
        "for v in array:\n",
        "  for t in tarray:\n",
        "    cosine = cx(v, t)\n",
        "    result.append(cosine)\n",
        "p = tf2.filter(['No','tweets'], axis=1)\n",
        "s = pd.Series(result)\n",
        "p['Cosine_similarity'] = s.values\n",
        "p.drop(p.loc[p['Cosine_similarity']==0].index, inplace=True)\n",
        "p[\"Rank\"] = p[\"Cosine_similarity\"].rank().astype(int)\n",
        "p.sort_values(\"Cosine_similarity\", inplace=True)\n",
        "p"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>Cosine_similarity</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>39 new cases in the Central African Republic \\...</td>\n",
              "      <td>0.138675</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Now playing on African Affairs Radio: if by da...</td>\n",
              "      <td>0.645497</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweets  Cosine_similarity  Rank\n",
              "71  39 new cases in the Central African Republic \\...           0.138675     1\n",
              "2   Now playing on African Affairs Radio: if by da...           0.645497     2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: https://github.com/s-neela/neela_INFO5731_spring2021/blob/main/Review.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}