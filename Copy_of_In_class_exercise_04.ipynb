{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of In-class-exercise-04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-neela/neela_INFO5731_spring2021/blob/main/Copy_of_In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0L3_CreM_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4859064e-1bc3-4706-d9d5-62659742bddc"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "# download the case file\n",
        "import requests\n",
        "def download_legalCaseDoc(url, file_name):\n",
        "  response = requests.get(url)\n",
        "  with open(file_name, 'wb') as fileWriter:\n",
        "    fileWriter.write(response.content)\n",
        "file_name = \"legal_case.txt\"\n",
        "download_legalCaseDoc(\"https://raw.githubusercontent.com/unt-iialab/info5731_spring2021/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt\", file_name)\n",
        "\n",
        "\n",
        "# 1.1 Basic feature extraction using text data'\n",
        "\n",
        "import numpy as np \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_file_content(legalCaseFile):\n",
        "  with open(legalCaseFile) as legalContentFile:\n",
        "    legalContent = legalContentFile.read()\n",
        "    return legalContent.replace('\\n','')\n",
        "\n",
        "def extract_sentences(legalContent):\n",
        "  rawlines = { line for line in legalContent.split('.')}\n",
        "  linesWithSentences = list(filter(None, rawlines))\n",
        "  return linesWithSentences\n",
        "\n",
        "def extract_charecters(word):\n",
        "  return list(word)\n",
        "\n",
        "def find_stopWords(words):\n",
        "  for word in words:\n",
        "    if word in en_stops:\n",
        "      yield word\n",
        "\n",
        "legalContent = extract_file_content(file_name)\n",
        "sentences = extract_sentences(legalContent)\n",
        "npSentences = np.array(sentences)\n",
        "npWords = np.concatenate(np.char.split(npSentences), axis=0) # Spread array \n",
        "stopWords = list(find_stopWords(npWords))\n",
        "npCharecters = np.concatenate([ extract_charecters(word) for word in npWords], axis=0)\n",
        "averageLengthOfWords = sum(len(word) for word in npWords)/len(npWords)\n",
        "special_charecters = find_special_charecters(legalContent)\n",
        "print('\\n# 1.1 Basic feature extraction using text data\\n')\n",
        "print(f'Number of Sentences : {len(npSentences)}')\n",
        "print(f'Nunmber of Words: {len(npWords)}')\n",
        "print(f'Number of Charecters: {len(npCharecters)}')\n",
        "print(f'Average Length Of Word: {averageLengthOfWords} , rounded to : {round(averageLengthOfWords)}')\n",
        "print(f'Number of stopwords: {len(stopWords)}') \n",
        "print(f'Number of special characters: {len(find_special_charecters(legalContent))}')\n",
        "print(f'Number of numerics: {len(find_numbers(legalContent))}')\n",
        "print(f'Number of uppercase words: {len(list(find_uppdercaseWords(npWords)))}')\n",
        "\n",
        "\n",
        "# 1.2 Basic Text Pre-processing of text data\n",
        "import string\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def print_data(dataFrame):\n",
        "  print(dataFrame['Sentence'].head())\n",
        "\n",
        "tokenized_sentences = nltk.tokenize.sent_tokenize(legalContent)\n",
        "dataFrame = pd.DataFrame(tokenized_sentences, columns=['Sentence'])\n",
        "\n",
        "print('\\nLower casing\\n')\n",
        "dataFrame['Sentence']= dataFrame['Sentence'].apply(lambda sentence: sentence.lower())\n",
        "print_data(dataFrame)\n",
        "\n",
        "print('\\nPunctuation removal\\n')\n",
        "dataFrame['Sentence']= dataFrame['Sentence'].apply(lambda sentence: sentence.translate(str.maketrans('', '', string.punctuation)))\n",
        "print_data(dataFrame)\n",
        "\n",
        "print('\\nStopwords removal\\n')\n",
        "freq=pd.Series(' '.join(dataFrame['Sentence']).split()).value_counts()[:10]\n",
        "dataFrame['Sentence'] = dataFrame['Sentence'].apply(lambda sentence: \" \".join(word for word in sentence.split() if word not in stopWords))\n",
        "print_data(dataFrame)\n",
        "\n",
        "print('\\nFrequent words removal\\n')\n",
        "freq_words=pd.Series(' '.join(dataFrame['Sentence']).split()).value_counts()[:10].index\n",
        "dataFrame['Sentence'] = dataFrame['Sentence'].apply(lambda sentence: \" \".join(word for word in sentence.split() if word not in freq_words))\n",
        "print_data(dataFrame)\n",
        "\n",
        "print('\\nRare words removal\\n')\n",
        "rare_words=pd.Series(' '.join(dataFrame['Sentence']).split()).value_counts()[-10:].index\n",
        "dataFrame['Sentence'] = dataFrame['Sentence'].apply(lambda sentence: \" \".join(word for word in sentence.split() if word not in rare_words))\n",
        "print_data(dataFrame)\n",
        "\n",
        "print('\\nSpelling correction\\n')\n",
        "from textblob import TextBlob\n",
        "dataFrame['Sentence'] = dataFrame['Sentence'].apply(lambda sentence: \" \".join([str(TextBlob(word).correct()) for word in sentence.split()]))\n",
        "print_data(dataFrame)\n",
        "\n",
        "print('\\nTokenization\\n')\n",
        "print(TextBlob(dataFrame['Sentence'][1]).words)\n",
        "\n",
        "print('\\nStemming\\n')\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "print(dataFrame['Sentence'].apply(lambda sentence: \" \".join([stemmer.stem(word) for word in sentence.split()])))\n",
        "\n",
        "print('\\Lemmatization\\n')\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "dataFrame['Sentence'] = dataFrame['Sentence'].apply(lambda sentence: \" \".join([Word(word).lemmatize() for word in sentence.split()]))\n",
        "print_data(dataFrame)\n",
        "\n",
        "\n",
        "#1.3 Save all the clean sentences to a csv file\n",
        "processed_file_name = 'processed_data.csv'\n",
        "print(f'\\nSaved into file : {processed_file_name}\\n')\n",
        "dataFrame.to_csv(processed_file_name, index=False)\n",
        "\n",
        "#1.4 Advance Text Processing\n",
        "print('\\nCalculate the term frequency of all the terms.\\n')\n",
        "tf = (dataFrame['Sentence'][1:2]).apply(lambda sentence:pd.value_counts(sentence.split())).sum(axis=0).reset_index()\n",
        "tf.columns=['Term','frequency']\n",
        "print(tf)\n",
        "\n",
        "print('\\nPrint out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\\n')\n",
        "print(TextBlob(dataFrame['Sentence'][10]).ngrams(1))\n",
        "print(TextBlob(dataFrame['Sentence'][10]).ngrams(2))\n",
        "print(TextBlob(dataFrame['Sentence'][10]).ngrams(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "# 1.1 Basic feature extraction using text data\n",
            "\n",
            "Number of Sentences : 255\n",
            "Nunmber of Words: 3599\n",
            "Number of Charecters: 16286\n",
            "Average Length Of Word: 4.525145873853848 , rounded to : 5\n",
            "Number of stopwords: 1670\n",
            "Number of special characters: 818\n",
            "Number of numerics: 156\n",
            "Number of uppercase words: 419\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "Lower casing\n",
            "\n",
            "0    5 ala. 740supreme court of alabama.adamsv.tann...\n",
            "1    west headnotes (2)[1]chattel mortgagescropsa g...\n",
            "2                                                  fa.\n",
            "3    on a growing crop, nor does such lien attach u...\n",
            "4    in november, 1840, an execution issued from th...\n",
            "Name: Sentence, dtype: object\n",
            "\n",
            "Punctuation removal\n",
            "\n",
            "0    5 ala 740supreme court of alabamaadamsvtanner ...\n",
            "1    west headnotes 21chattel mortgagescropsa growi...\n",
            "2                                                   fa\n",
            "3    on a growing crop nor does such lien attach un...\n",
            "4    in november 1840 an execution issued from the ...\n",
            "Name: Sentence, dtype: object\n",
            "\n",
            "Stopwords removal\n",
            "\n",
            "0    5 ala 740supreme court alabamaadamsvtanner hor...\n",
            "1    west headnotes 21chattel mortgagescropsa growi...\n",
            "2                                                   fa\n",
            "3    growing crop lien attach crop gathered5 cases ...\n",
            "4    november 1840 execution issued circuit court s...\n",
            "Name: Sentence, dtype: object\n",
            "\n",
            "Frequent words removal\n",
            "\n",
            "0    5 ala 740supreme alabamaadamsvtanner hortonjun...\n",
            "1    west headnotes 21chattel mortgagescropsa growi...\n",
            "2                                                   fa\n",
            "3    growing attach gathered5 cases cite headnote1 ...\n",
            "4    november 1840 issued circuit sumter suit plain...\n",
            "Name: Sentence, dtype: object\n",
            "\n",
            "Rare words removal\n",
            "\n",
            "0    5 ala 740supreme alabamaadamsvtanner hortonjun...\n",
            "1    west headnotes 21chattel mortgagescropsa growi...\n",
            "2                                                   fa\n",
            "3    growing attach gathered5 cases cite headnote1 ...\n",
            "4    november 1840 issued circuit sumter suit plain...\n",
            "Name: Sentence, dtype: object\n",
            "\n",
            "Spelling correction\n",
            "\n",
            "\n",
            "Tokenization\n",
            "\n",
            "['west', 'headnotes', '21chattel', 'mortgagescropsa', 'growing', 'existence', 'subjectmatter', 'mortgage', 'passes', 'interest', 'vest', 'possession', 'either', 'immediately', 'future', 'time4', 'cases', 'cite', 'headnote2creditors', '’', 'remedieslien', 'priorityunder', 'st1821', 'prohibiting', 'gathered', 'attaches', 'favor', 'fi']\n",
            "\n",
            "Stemming\n",
            "\n",
            "0      5 ala 740suprem alabamaadamsvtann hortonjun te...\n",
            "1      west headnot 21chattel mortgagescropsa grow ex...\n",
            "2                                                     fa\n",
            "3      grow attach gathered5 case cite headnote1 tria...\n",
            "4      novemb 1840 issu circuit sumter suit plaintiff...\n",
            "                             ...                        \n",
            "141                                perkin mayfield5 port\n",
            "142    182 ala 1837on writ error circuit tuskaloosaca...\n",
            "143                                stewart doughty9 john\n",
            "144                                      108 nysup 1812a\n",
            "145    let b farm six year agre “to render yield pay ...\n",
            "Name: Sentence, Length: 146, dtype: object\n",
            "\\Lemmatization\n",
            "\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "0    5 ala 740supreme alabamaadamsvtanner hortonjun...\n",
            "1    west headnotes 21chattel mortgagescropsa growi...\n",
            "2                                                   fa\n",
            "3    growing attach gathered5 case cite headnote1 t...\n",
            "4    november 1840 issued circuit sumter suit plain...\n",
            "Name: Sentence, dtype: object\n",
            "\n",
            "Saved into file : processed_data.csv\n",
            "\n",
            "\n",
            "Calculate the term frequency of all the terms.\n",
            "\n",
            "                   Term  frequency\n",
            "0             21chattel          1\n",
            "1               attache          1\n",
            "2       mortgagescropsa          1\n",
            "3               growing          1\n",
            "4                  west          1\n",
            "5              mortgage          1\n",
            "6                future          1\n",
            "7                 time4          1\n",
            "8                 favor          1\n",
            "9            possession          1\n",
            "10             interest          1\n",
            "11                 case          1\n",
            "12                   fi          1\n",
            "13               either          1\n",
            "14  headnote2creditors’          1\n",
            "15                 cite          1\n",
            "16               st1821          1\n",
            "17        priorityunder          1\n",
            "18            existence          1\n",
            "19          prohibiting          1\n",
            "20                 vest          1\n",
            "21            headnotes          1\n",
            "22          immediately          1\n",
            "23             gathered          1\n",
            "24         remedieslien          1\n",
            "25                 pass          1\n",
            "26        subjectmatter          1\n",
            "\n",
            "Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n",
            "\n",
            "[WordList(['came']), WordList(['tennessee']), WordList(['resided']), WordList(['first']), WordList(['september']), WordList(['1840']), WordList(['bringing']), WordList(['three']), WordList(['four']), WordList(['white']), WordList(['laborer']), WordList(['took']), WordList(['possession']), WordList(['slave']), WordList(['latter']), WordList(['white']), WordList(['laborer']), WordList(['gathered']), WordList(['cotton']), WordList(['prepared']), WordList(['market']), WordList(['levied']), WordList(['warehouse']), WordList(['gainesville'])]\n",
            "[WordList(['came', 'tennessee']), WordList(['tennessee', 'resided']), WordList(['resided', 'first']), WordList(['first', 'september']), WordList(['september', '1840']), WordList(['1840', 'bringing']), WordList(['bringing', 'three']), WordList(['three', 'four']), WordList(['four', 'white']), WordList(['white', 'laborer']), WordList(['laborer', 'took']), WordList(['took', 'possession']), WordList(['possession', 'slave']), WordList(['slave', 'latter']), WordList(['latter', 'white']), WordList(['white', 'laborer']), WordList(['laborer', 'gathered']), WordList(['gathered', 'cotton']), WordList(['cotton', 'prepared']), WordList(['prepared', 'market']), WordList(['market', 'levied']), WordList(['levied', 'warehouse']), WordList(['warehouse', 'gainesville'])]\n",
            "[WordList(['came', 'tennessee', 'resided']), WordList(['tennessee', 'resided', 'first']), WordList(['resided', 'first', 'september']), WordList(['first', 'september', '1840']), WordList(['september', '1840', 'bringing']), WordList(['1840', 'bringing', 'three']), WordList(['bringing', 'three', 'four']), WordList(['three', 'four', 'white']), WordList(['four', 'white', 'laborer']), WordList(['white', 'laborer', 'took']), WordList(['laborer', 'took', 'possession']), WordList(['took', 'possession', 'slave']), WordList(['possession', 'slave', 'latter']), WordList(['slave', 'latter', 'white']), WordList(['latter', 'white', 'laborer']), WordList(['white', 'laborer', 'gathered']), WordList(['laborer', 'gathered', 'cotton']), WordList(['gathered', 'cotton', 'prepared']), WordList(['cotton', 'prepared', 'market']), WordList(['prepared', 'market', 'levied']), WordList(['market', 'levied', 'warehouse']), WordList(['levied', 'warehouse', 'gainesville'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6fVhOfFmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2bd82a-9c0c-416d-bf3a-f3e3f7bf1838"
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "\n",
        "def remove_leadingZeros(ip):\n",
        "  result = re.sub(r'\\b0+(\\d)', r'\\1', ip)\n",
        "  return result\n",
        "\n",
        "ip = \"260.08.094.109\"\n",
        "\n",
        "print(remove_leadingZeros(ip))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJpDx9gjbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e43dd8-bba2-4fbc-eb96-4c23e8d52532"
      },
      "source": [
        "# Write your code here\r\n",
        "import re\r\n",
        "\r\n",
        "def valid_year(year):\r\n",
        "  if year and year.isdigit():\r\n",
        "    if int(year) >=1900 and int(year) <=2020:\r\n",
        "      return int(year)  #return an integer\r\n",
        "\r\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\r\n",
        "all_4digitNumbers = re.findall(r'\\d{4}', sentence)\r\n",
        "years = [number for number in all_4digitNumbers if valid_year(number)]\r\n",
        "print(years)\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2010', '2010', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}